<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Python on Iverson&#39;s blog</title>
    <link>http://answerkobe.github.io/tags/python/</link>
    <description>Recent content in Python on Iverson&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>cn-zh</language>
    <lastBuildDate>Thu, 13 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="http://answerkobe.github.io/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>神经网络基础理论与简单实践</title>
      <link>http://answerkobe.github.io/posts/basic-theory-and-simple-practice-of-neural-network/</link>
      <pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>http://answerkobe.github.io/posts/basic-theory-and-simple-practice-of-neural-network/</guid>
      <description>机器学习（Machine Learning, ML） 在了解神经网络之前，我们需要先知道机器学习和神经网络之间的关系。
机器学习是人工智能中的一个分支， 主要研究如何让计算机模拟人类的学习行为来获得知识和技能，在实践上就是通过让泛型算法（Generic Algorithm，和 C/C++ 中的泛型算法不是一个东西）自己在输入的数据上建立逻辑模型，并通过该模型来得到目标结论。
泛型算法能处理的问题取决于我们所定义好的输入/输出的数据格式，而处理问题的条件和逻辑，则由泛型算法通过对输入数据进行分析学习构建处理逻辑参数，构建后的逻辑也能适应非预设数据的处理。神经网络属于机器学习泛型算法中的一种实现方案。
根据泛型算法的学习方式，也可以将机器学习分为监督式学习、弱监督学习、半监督学习、非监督式学习、迁移学习、强化学习等。 人工智能中许多维度的分类都存在交叉关系，因此通常不会为任何一个方法论进行确切的归类。 神经网络结构 【概念】机器学习中的神经网络模型是一种模仿生物神经网络结构和功能的模型，因此也被称为人工神经网络或类神经网络。人工神经网络由多个人工神经元（处理单元）以及传递信号的链接形成拓扑结构，由于泛型算法能处理的问题取决于输入/输出的数据格式，因此神经网络基本会分为3个层：
Input Layer：输入层，用于接收外界数据，节点数量根据输入的数据类型决定 Hidden Layer：隐含层，负责对输入层提供的数据进行信息处理、信息转化，通常这一层会有多个层次，每层会将处理结果向后面传递。 Output Layer：输出层，将隐含层提供的输出信息转化层最终结果，节点数量根据输出的结构类型决定 【生物知识点复习】生物神经元通常具有多个树突，树突用于接收信号，接收的信号在细胞体内整合产生阶梯性生电，而轴突用于传递信号，当细胞体的电位影响达到一定的阈值，则代表这个神经元被激活，激活的神经元会产生神经冲动（电脉冲），通过轴突传导出去，轴突尾端有许多和其他神经元树突产生连接的突触，电信号会通过这些突触传递给其他神经元，突触在一次突触事件中产生的电位幅度与突触的强度有关。
人工神经元也是模拟生物神经元的结构和特性，下面是一个典型的人工神经元结构，以及人工神经元和生物神经元中各个行为的对照表。
生物神经元 人工神经元 输入，即上一个神经元轴突传递过来的电信号 x𝑖 也就是下一个神经元的输出 y 突触强度 w𝑗 权重 树突接收到的电信号 x𝑖 * w𝑗 信号积累，阶段性生电 ∑(x𝑖 * w𝑗) 神经元激活 𝑓(·) 激活函数 轴突电信号传递 y 也就是下一个神经元的输入 x𝑖 线性回归 从上面的对照表可以看出，一个神经元对上一层的输入处理其实就是将各个输入值加权求和，本质上就是一个线性回归 𝑓(𝑥;𝑤) = 𝑤1𝑥1 + 𝑤2𝑥2 + ⋯ + 𝑤D𝑥D + 𝚋。线性回归模型是机器学习中最基础最广泛的模型，主要用于分析自变量和因变量之间的关系。 为什么使用的是线性回归呢？ 线性回归可以用来描述自变量和因变量之间的关系，机器学习中大部分问题都是分析数据里特征的关系来建立模型，因此在机器学习中很多问题都可以转换为线性回归问题来处理。 例如我们有如下左图关于面积和房价关系的数据，那么可以用一个一元线性回归模型来拟合这些数据，从而得到一个可以根据面积来预估房价的模型。当房屋特征变多时，也可以根据回归函数的参数建立一个没有 Hidden Layer 的神经网络。
面积和房价的线性拟合 多个房屋特征的神经网络 有时候特征向量与因变量并不是简单的线性关系，例如我们给的不是面积，而是房子的长宽，那么将会存在特征之间相乘的计算逻辑。或者有时候特征向量之间可能也存在关系，例如加上时间维度来预测未来的房价，那么其他特征带来的效果可能会跟着时间变化发生变化。如果我们在各个线性回归的关系上再次建立回归模型，那么工作量和计算量将会特别高，但多层的神经网络来处理这个问题就非常方便。
神经网络本身是由许多节点组成的，每个节点的输入输出都可以认为是一次线性转化，因此可以认为神经网络会将问题分为多个子问题来处理，不同纬度的问题会被分到各个层级，同一纬度的子问题会被分到各个神经元。每个神经元利用线性回归来对输入数据的特征进行线性转换（这个过程也称为特征提取），将子问题分析结果反馈给下一层级（父问题）继续处理。
没有 Hidden Layer 的神经网络只能用于表示线性回归函数，但多层的网络则可以在线性回归上建立更高纬度的模型。下面是 The Number of Hidden Layers 中总结的常见层数体系结构的功能：</description>
    </item>
    
  </channel>
</rss>
