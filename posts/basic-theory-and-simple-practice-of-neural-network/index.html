<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=Content-Security-Policy content="upgrade-insecure-requests"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>神经网络基础理论与简单实践 | Iverson's blog</title>
<meta name=keywords content="Neural Network,Python"><meta name=description content="机器学习（Machine Learning, ML） 在了解神经网络之前，我们需要先知道机器学习和神经网络之间的关系。
机器学习是人工智能中的一个分支， 主要研究如何让计算机模拟人类的学习行为来获得知识和技能，在实践上就是通过让泛型算法（Generic Algorithm，和 C/C++ 中的泛型算法不是一个东西）自己在输入的数据上建立逻辑模型，并通过该模型来得到目标结论。
泛型算法能处理的问题取决于我们所定义好的输入/输出的数据格式，而处理问题的条件和逻辑，则由泛型算法通过对输入数据进行分析学习构建处理逻辑参数，构建后的逻辑也能适应非预设数据的处理。神经网络属于机器学习泛型算法中的一种实现方案。
根据泛型算法的学习方式，也可以将机器学习分为监督式学习、弱监督学习、半监督学习、非监督式学习、迁移学习、强化学习等。 人工智能中许多维度的分类都存在交叉关系，因此通常不会为任何一个方法论进行确切的归类。 神经网络结构 【概念】机器学习中的神经网络模型是一种模仿生物神经网络结构和功能的模型，因此也被称为人工神经网络或类神经网络。人工神经网络由多个人工神经元（处理单元）以及传递信号的链接形成拓扑结构，由于泛型算法能处理的问题取决于输入/输出的数据格式，因此神经网络基本会分为3个层：
Input Layer：输入层，用于接收外界数据，节点数量根据输入的数据类型决定 Hidden Layer：隐含层，负责对输入层提供的数据进行信息处理、信息转化，通常这一层会有多个层次，每层会将处理结果向后面传递。 Output Layer：输出层，将隐含层提供的输出信息转化层最终结果，节点数量根据输出的结构类型决定 【生物知识点复习】生物神经元通常具有多个树突，树突用于接收信号，接收的信号在细胞体内整合产生阶梯性生电，而轴突用于传递信号，当细胞体的电位影响达到一定的阈值，则代表这个神经元被激活，激活的神经元会产生神经冲动（电脉冲），通过轴突传导出去，轴突尾端有许多和其他神经元树突产生连接的突触，电信号会通过这些突触传递给其他神经元，突触在一次突触事件中产生的电位幅度与突触的强度有关。
人工神经元也是模拟生物神经元的结构和特性，下面是一个典型的人工神经元结构，以及人工神经元和生物神经元中各个行为的对照表。
生物神经元 人工神经元 输入，即上一个神经元轴突传递过来的电信号 x𝑖 也就是下一个神经元的输出 y 突触强度 w𝑗 权重 树突接收到的电信号 x𝑖 * w𝑗 信号积累，阶段性生电 ∑(x𝑖 * w𝑗) 神经元激活 𝑓(·) 激活函数 轴突电信号传递 y 也就是下一个神经元的输入 x𝑖 线性回归 从上面的对照表可以看出，一个神经元对上一层的输入处理其实就是将各个输入值加权求和，本质上就是一个线性回归 𝑓(𝑥;𝑤) = 𝑤1𝑥1 + 𝑤2𝑥2 + ⋯ + 𝑤D𝑥D + 𝚋。线性回归模型是机器学习中最基础最广泛的模型，主要用于分析自变量和因变量之间的关系。 为什么使用的是线性回归呢？ 线性回归可以用来描述自变量和因变量之间的关系，机器学习中大部分问题都是分析数据里特征的关系来建立模型，因此在机器学习中很多问题都可以转换为线性回归问题来处理。 例如我们有如下左图关于面积和房价关系的数据，那么可以用一个一元线性回归模型来拟合这些数据，从而得到一个可以根据面积来预估房价的模型。当房屋特征变多时，也可以根据回归函数的参数建立一个没有 Hidden Layer 的神经网络。
面积和房价的线性拟合 多个房屋特征的神经网络 有时候特征向量与因变量并不是简单的线性关系，例如我们给的不是面积，而是房子的长宽，那么将会存在特征之间相乘的计算逻辑。或者有时候特征向量之间可能也存在关系，例如加上时间维度来预测未来的房价，那么其他特征带来的效果可能会跟着时间变化发生变化。如果我们在各个线性回归的关系上再次建立回归模型，那么工作量和计算量将会特别高，但多层的神经网络来处理这个问题就非常方便。
神经网络本身是由许多节点组成的，每个节点的输入输出都可以认为是一次线性转化，因此可以认为神经网络会将问题分为多个子问题来处理，不同纬度的问题会被分到各个层级，同一纬度的子问题会被分到各个神经元。每个神经元利用线性回归来对输入数据的特征进行线性转换（这个过程也称为特征提取），将子问题分析结果反馈给下一层级（父问题）继续处理。
没有 Hidden Layer 的神经网络只能用于表示线性回归函数，但多层的网络则可以在线性回归上建立更高纬度的模型。下面是 The Number of Hidden Layers 中总结的常见层数体系结构的功能："><meta name=author content><link rel=canonical href=http://answerkobe.github.io/posts/basic-theory-and-simple-practice-of-neural-network/><link crossorigin=anonymous href=/assets/css/stylesheet.81e9c84b86321fe9cfdf9f01a2f8060920d09e0fec45cb91f131cfed0284007e.css integrity="sha256-genIS4YyH+nP358BovgGCSDQng/sRcuR8THP7QKEAH4=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=http://answerkobe.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://answerkobe.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://answerkobe.github.io/favicon-32x32.png><link rel=apple-touch-icon href=http://answerkobe.github.io/apple-touch-icon.png><link rel=mask-icon href=http://answerkobe.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y4112QEXZJ"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y4112QEXZJ",{anonymize_ip:!1})}</script><meta property="og:title" content="神经网络基础理论与简单实践"><meta property="og:description" content="机器学习（Machine Learning, ML） 在了解神经网络之前，我们需要先知道机器学习和神经网络之间的关系。
机器学习是人工智能中的一个分支， 主要研究如何让计算机模拟人类的学习行为来获得知识和技能，在实践上就是通过让泛型算法（Generic Algorithm，和 C/C++ 中的泛型算法不是一个东西）自己在输入的数据上建立逻辑模型，并通过该模型来得到目标结论。
泛型算法能处理的问题取决于我们所定义好的输入/输出的数据格式，而处理问题的条件和逻辑，则由泛型算法通过对输入数据进行分析学习构建处理逻辑参数，构建后的逻辑也能适应非预设数据的处理。神经网络属于机器学习泛型算法中的一种实现方案。
根据泛型算法的学习方式，也可以将机器学习分为监督式学习、弱监督学习、半监督学习、非监督式学习、迁移学习、强化学习等。 人工智能中许多维度的分类都存在交叉关系，因此通常不会为任何一个方法论进行确切的归类。 神经网络结构 【概念】机器学习中的神经网络模型是一种模仿生物神经网络结构和功能的模型，因此也被称为人工神经网络或类神经网络。人工神经网络由多个人工神经元（处理单元）以及传递信号的链接形成拓扑结构，由于泛型算法能处理的问题取决于输入/输出的数据格式，因此神经网络基本会分为3个层：
Input Layer：输入层，用于接收外界数据，节点数量根据输入的数据类型决定 Hidden Layer：隐含层，负责对输入层提供的数据进行信息处理、信息转化，通常这一层会有多个层次，每层会将处理结果向后面传递。 Output Layer：输出层，将隐含层提供的输出信息转化层最终结果，节点数量根据输出的结构类型决定 【生物知识点复习】生物神经元通常具有多个树突，树突用于接收信号，接收的信号在细胞体内整合产生阶梯性生电，而轴突用于传递信号，当细胞体的电位影响达到一定的阈值，则代表这个神经元被激活，激活的神经元会产生神经冲动（电脉冲），通过轴突传导出去，轴突尾端有许多和其他神经元树突产生连接的突触，电信号会通过这些突触传递给其他神经元，突触在一次突触事件中产生的电位幅度与突触的强度有关。
人工神经元也是模拟生物神经元的结构和特性，下面是一个典型的人工神经元结构，以及人工神经元和生物神经元中各个行为的对照表。
生物神经元 人工神经元 输入，即上一个神经元轴突传递过来的电信号 x𝑖 也就是下一个神经元的输出 y 突触强度 w𝑗 权重 树突接收到的电信号 x𝑖 * w𝑗 信号积累，阶段性生电 ∑(x𝑖 * w𝑗) 神经元激活 𝑓(·) 激活函数 轴突电信号传递 y 也就是下一个神经元的输入 x𝑖 线性回归 从上面的对照表可以看出，一个神经元对上一层的输入处理其实就是将各个输入值加权求和，本质上就是一个线性回归 𝑓(𝑥;𝑤) = 𝑤1𝑥1 + 𝑤2𝑥2 + ⋯ + 𝑤D𝑥D + 𝚋。线性回归模型是机器学习中最基础最广泛的模型，主要用于分析自变量和因变量之间的关系。 为什么使用的是线性回归呢？ 线性回归可以用来描述自变量和因变量之间的关系，机器学习中大部分问题都是分析数据里特征的关系来建立模型，因此在机器学习中很多问题都可以转换为线性回归问题来处理。 例如我们有如下左图关于面积和房价关系的数据，那么可以用一个一元线性回归模型来拟合这些数据，从而得到一个可以根据面积来预估房价的模型。当房屋特征变多时，也可以根据回归函数的参数建立一个没有 Hidden Layer 的神经网络。
面积和房价的线性拟合 多个房屋特征的神经网络 有时候特征向量与因变量并不是简单的线性关系，例如我们给的不是面积，而是房子的长宽，那么将会存在特征之间相乘的计算逻辑。或者有时候特征向量之间可能也存在关系，例如加上时间维度来预测未来的房价，那么其他特征带来的效果可能会跟着时间变化发生变化。如果我们在各个线性回归的关系上再次建立回归模型，那么工作量和计算量将会特别高，但多层的神经网络来处理这个问题就非常方便。
神经网络本身是由许多节点组成的，每个节点的输入输出都可以认为是一次线性转化，因此可以认为神经网络会将问题分为多个子问题来处理，不同纬度的问题会被分到各个层级，同一纬度的子问题会被分到各个神经元。每个神经元利用线性回归来对输入数据的特征进行线性转换（这个过程也称为特征提取），将子问题分析结果反馈给下一层级（父问题）继续处理。
没有 Hidden Layer 的神经网络只能用于表示线性回归函数，但多层的网络则可以在线性回归上建立更高纬度的模型。下面是 The Number of Hidden Layers 中总结的常见层数体系结构的功能："><meta property="og:type" content="article"><meta property="og:url" content="http://answerkobe.github.io/posts/basic-theory-and-simple-practice-of-neural-network/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-04-13T00:00:00+00:00"><meta property="article:modified_time" content="2023-04-13T00:00:00+00:00"><meta property="og:site_name" content="Iverson"><meta name=twitter:card content="summary"><meta name=twitter:title content="神经网络基础理论与简单实践"><meta name=twitter:description content="机器学习（Machine Learning, ML） 在了解神经网络之前，我们需要先知道机器学习和神经网络之间的关系。
机器学习是人工智能中的一个分支， 主要研究如何让计算机模拟人类的学习行为来获得知识和技能，在实践上就是通过让泛型算法（Generic Algorithm，和 C/C++ 中的泛型算法不是一个东西）自己在输入的数据上建立逻辑模型，并通过该模型来得到目标结论。
泛型算法能处理的问题取决于我们所定义好的输入/输出的数据格式，而处理问题的条件和逻辑，则由泛型算法通过对输入数据进行分析学习构建处理逻辑参数，构建后的逻辑也能适应非预设数据的处理。神经网络属于机器学习泛型算法中的一种实现方案。
根据泛型算法的学习方式，也可以将机器学习分为监督式学习、弱监督学习、半监督学习、非监督式学习、迁移学习、强化学习等。 人工智能中许多维度的分类都存在交叉关系，因此通常不会为任何一个方法论进行确切的归类。 神经网络结构 【概念】机器学习中的神经网络模型是一种模仿生物神经网络结构和功能的模型，因此也被称为人工神经网络或类神经网络。人工神经网络由多个人工神经元（处理单元）以及传递信号的链接形成拓扑结构，由于泛型算法能处理的问题取决于输入/输出的数据格式，因此神经网络基本会分为3个层：
Input Layer：输入层，用于接收外界数据，节点数量根据输入的数据类型决定 Hidden Layer：隐含层，负责对输入层提供的数据进行信息处理、信息转化，通常这一层会有多个层次，每层会将处理结果向后面传递。 Output Layer：输出层，将隐含层提供的输出信息转化层最终结果，节点数量根据输出的结构类型决定 【生物知识点复习】生物神经元通常具有多个树突，树突用于接收信号，接收的信号在细胞体内整合产生阶梯性生电，而轴突用于传递信号，当细胞体的电位影响达到一定的阈值，则代表这个神经元被激活，激活的神经元会产生神经冲动（电脉冲），通过轴突传导出去，轴突尾端有许多和其他神经元树突产生连接的突触，电信号会通过这些突触传递给其他神经元，突触在一次突触事件中产生的电位幅度与突触的强度有关。
人工神经元也是模拟生物神经元的结构和特性，下面是一个典型的人工神经元结构，以及人工神经元和生物神经元中各个行为的对照表。
生物神经元 人工神经元 输入，即上一个神经元轴突传递过来的电信号 x𝑖 也就是下一个神经元的输出 y 突触强度 w𝑗 权重 树突接收到的电信号 x𝑖 * w𝑗 信号积累，阶段性生电 ∑(x𝑖 * w𝑗) 神经元激活 𝑓(·) 激活函数 轴突电信号传递 y 也就是下一个神经元的输入 x𝑖 线性回归 从上面的对照表可以看出，一个神经元对上一层的输入处理其实就是将各个输入值加权求和，本质上就是一个线性回归 𝑓(𝑥;𝑤) = 𝑤1𝑥1 + 𝑤2𝑥2 + ⋯ + 𝑤D𝑥D + 𝚋。线性回归模型是机器学习中最基础最广泛的模型，主要用于分析自变量和因变量之间的关系。 为什么使用的是线性回归呢？ 线性回归可以用来描述自变量和因变量之间的关系，机器学习中大部分问题都是分析数据里特征的关系来建立模型，因此在机器学习中很多问题都可以转换为线性回归问题来处理。 例如我们有如下左图关于面积和房价关系的数据，那么可以用一个一元线性回归模型来拟合这些数据，从而得到一个可以根据面积来预估房价的模型。当房屋特征变多时，也可以根据回归函数的参数建立一个没有 Hidden Layer 的神经网络。
面积和房价的线性拟合 多个房屋特征的神经网络 有时候特征向量与因变量并不是简单的线性关系，例如我们给的不是面积，而是房子的长宽，那么将会存在特征之间相乘的计算逻辑。或者有时候特征向量之间可能也存在关系，例如加上时间维度来预测未来的房价，那么其他特征带来的效果可能会跟着时间变化发生变化。如果我们在各个线性回归的关系上再次建立回归模型，那么工作量和计算量将会特别高，但多层的神经网络来处理这个问题就非常方便。
神经网络本身是由许多节点组成的，每个节点的输入输出都可以认为是一次线性转化，因此可以认为神经网络会将问题分为多个子问题来处理，不同纬度的问题会被分到各个层级，同一纬度的子问题会被分到各个神经元。每个神经元利用线性回归来对输入数据的特征进行线性转换（这个过程也称为特征提取），将子问题分析结果反馈给下一层级（父问题）继续处理。
没有 Hidden Layer 的神经网络只能用于表示线性回归函数，但多层的网络则可以在线性回归上建立更高纬度的模型。下面是 The Number of Hidden Layers 中总结的常见层数体系结构的功能："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://answerkobe.github.io/posts/"},{"@type":"ListItem","position":2,"name":"神经网络基础理论与简单实践","item":"http://answerkobe.github.io/posts/basic-theory-and-simple-practice-of-neural-network/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"神经网络基础理论与简单实践","name":"神经网络基础理论与简单实践","description":"机器学习（Machine Learning, ML） 在了解神经网络之前，我们需要先知道机器学习和神经网络之间的关系。\n机器学习是人工智能中的一个分支， 主要研究如何让计算机模拟人类的学习行为来获得知识和技能，在实践上就是通过让泛型算法（Generic Algorithm，和 C/C++ 中的泛型算法不是一个东西）自己在输入的数据上建立逻辑模型，并通过该模型来得到目标结论。\n泛型算法能处理的问题取决于我们所定义好的输入/输出的数据格式，而处理问题的条件和逻辑，则由泛型算法通过对输入数据进行分析学习构建处理逻辑参数，构建后的逻辑也能适应非预设数据的处理。神经网络属于机器学习泛型算法中的一种实现方案。\n根据泛型算法的学习方式，也可以将机器学习分为监督式学习、弱监督学习、半监督学习、非监督式学习、迁移学习、强化学习等。 人工智能中许多维度的分类都存在交叉关系，因此通常不会为任何一个方法论进行确切的归类。 神经网络结构 【概念】机器学习中的神经网络模型是一种模仿生物神经网络结构和功能的模型，因此也被称为人工神经网络或类神经网络。人工神经网络由多个人工神经元（处理单元）以及传递信号的链接形成拓扑结构，由于泛型算法能处理的问题取决于输入/输出的数据格式，因此神经网络基本会分为3个层：\nInput Layer：输入层，用于接收外界数据，节点数量根据输入的数据类型决定 Hidden Layer：隐含层，负责对输入层提供的数据进行信息处理、信息转化，通常这一层会有多个层次，每层会将处理结果向后面传递。 Output Layer：输出层，将隐含层提供的输出信息转化层最终结果，节点数量根据输出的结构类型决定 【生物知识点复习】生物神经元通常具有多个树突，树突用于接收信号，接收的信号在细胞体内整合产生阶梯性生电，而轴突用于传递信号，当细胞体的电位影响达到一定的阈值，则代表这个神经元被激活，激活的神经元会产生神经冲动（电脉冲），通过轴突传导出去，轴突尾端有许多和其他神经元树突产生连接的突触，电信号会通过这些突触传递给其他神经元，突触在一次突触事件中产生的电位幅度与突触的强度有关。\n人工神经元也是模拟生物神经元的结构和特性，下面是一个典型的人工神经元结构，以及人工神经元和生物神经元中各个行为的对照表。\n生物神经元 人工神经元 输入，即上一个神经元轴突传递过来的电信号 x𝑖 也就是下一个神经元的输出 y 突触强度 w𝑗 权重 树突接收到的电信号 x𝑖 * w𝑗 信号积累，阶段性生电 ∑(x𝑖 * w𝑗) 神经元激活 𝑓(·) 激活函数 轴突电信号传递 y 也就是下一个神经元的输入 x𝑖 线性回归 从上面的对照表可以看出，一个神经元对上一层的输入处理其实就是将各个输入值加权求和，本质上就是一个线性回归 𝑓(𝑥;𝑤) = 𝑤1𝑥1 + 𝑤2𝑥2 + ⋯ + 𝑤D𝑥D + 𝚋。线性回归模型是机器学习中最基础最广泛的模型，主要用于分析自变量和因变量之间的关系。 为什么使用的是线性回归呢？ 线性回归可以用来描述自变量和因变量之间的关系，机器学习中大部分问题都是分析数据里特征的关系来建立模型，因此在机器学习中很多问题都可以转换为线性回归问题来处理。 例如我们有如下左图关于面积和房价关系的数据，那么可以用一个一元线性回归模型来拟合这些数据，从而得到一个可以根据面积来预估房价的模型。当房屋特征变多时，也可以根据回归函数的参数建立一个没有 Hidden Layer 的神经网络。\n面积和房价的线性拟合 多个房屋特征的神经网络 有时候特征向量与因变量并不是简单的线性关系，例如我们给的不是面积，而是房子的长宽，那么将会存在特征之间相乘的计算逻辑。或者有时候特征向量之间可能也存在关系，例如加上时间维度来预测未来的房价，那么其他特征带来的效果可能会跟着时间变化发生变化。如果我们在各个线性回归的关系上再次建立回归模型，那么工作量和计算量将会特别高，但多层的神经网络来处理这个问题就非常方便。\n神经网络本身是由许多节点组成的，每个节点的输入输出都可以认为是一次线性转化，因此可以认为神经网络会将问题分为多个子问题来处理，不同纬度的问题会被分到各个层级，同一纬度的子问题会被分到各个神经元。每个神经元利用线性回归来对输入数据的特征进行线性转换（这个过程也称为特征提取），将子问题分析结果反馈给下一层级（父问题）继续处理。\n没有 Hidden Layer 的神经网络只能用于表示线性回归函数，但多层的网络则可以在线性回归上建立更高纬度的模型。下面是 The Number of Hidden Layers 中总结的常见层数体系结构的功能：","keywords":["Neural Network","Python"],"articleBody":"机器学习（Machine Learning, ML） 在了解神经网络之前，我们需要先知道机器学习和神经网络之间的关系。\n机器学习是人工智能中的一个分支， 主要研究如何让计算机模拟人类的学习行为来获得知识和技能，在实践上就是通过让泛型算法（Generic Algorithm，和 C/C++ 中的泛型算法不是一个东西）自己在输入的数据上建立逻辑模型，并通过该模型来得到目标结论。\n泛型算法能处理的问题取决于我们所定义好的输入/输出的数据格式，而处理问题的条件和逻辑，则由泛型算法通过对输入数据进行分析学习构建处理逻辑参数，构建后的逻辑也能适应非预设数据的处理。神经网络属于机器学习泛型算法中的一种实现方案。\n根据泛型算法的学习方式，也可以将机器学习分为监督式学习、弱监督学习、半监督学习、非监督式学习、迁移学习、强化学习等。 人工智能中许多维度的分类都存在交叉关系，因此通常不会为任何一个方法论进行确切的归类。 神经网络结构 【概念】机器学习中的神经网络模型是一种模仿生物神经网络结构和功能的模型，因此也被称为人工神经网络或类神经网络。人工神经网络由多个人工神经元（处理单元）以及传递信号的链接形成拓扑结构，由于泛型算法能处理的问题取决于输入/输出的数据格式，因此神经网络基本会分为3个层：\nInput Layer：输入层，用于接收外界数据，节点数量根据输入的数据类型决定 Hidden Layer：隐含层，负责对输入层提供的数据进行信息处理、信息转化，通常这一层会有多个层次，每层会将处理结果向后面传递。 Output Layer：输出层，将隐含层提供的输出信息转化层最终结果，节点数量根据输出的结构类型决定 【生物知识点复习】生物神经元通常具有多个树突，树突用于接收信号，接收的信号在细胞体内整合产生阶梯性生电，而轴突用于传递信号，当细胞体的电位影响达到一定的阈值，则代表这个神经元被激活，激活的神经元会产生神经冲动（电脉冲），通过轴突传导出去，轴突尾端有许多和其他神经元树突产生连接的突触，电信号会通过这些突触传递给其他神经元，突触在一次突触事件中产生的电位幅度与突触的强度有关。\n人工神经元也是模拟生物神经元的结构和特性，下面是一个典型的人工神经元结构，以及人工神经元和生物神经元中各个行为的对照表。\n生物神经元 人工神经元 输入，即上一个神经元轴突传递过来的电信号 x𝑖 也就是下一个神经元的输出 y 突触强度 w𝑗 权重 树突接收到的电信号 x𝑖 * w𝑗 信号积累，阶段性生电 ∑(x𝑖 * w𝑗) 神经元激活 𝑓(·) 激活函数 轴突电信号传递 y 也就是下一个神经元的输入 x𝑖 线性回归 从上面的对照表可以看出，一个神经元对上一层的输入处理其实就是将各个输入值加权求和，本质上就是一个线性回归 𝑓(𝑥;𝑤) = 𝑤1𝑥1 + 𝑤2𝑥2 + ⋯ + 𝑤D𝑥D + 𝚋。线性回归模型是机器学习中最基础最广泛的模型，主要用于分析自变量和因变量之间的关系。 为什么使用的是线性回归呢？ 线性回归可以用来描述自变量和因变量之间的关系，机器学习中大部分问题都是分析数据里特征的关系来建立模型，因此在机器学习中很多问题都可以转换为线性回归问题来处理。 例如我们有如下左图关于面积和房价关系的数据，那么可以用一个一元线性回归模型来拟合这些数据，从而得到一个可以根据面积来预估房价的模型。当房屋特征变多时，也可以根据回归函数的参数建立一个没有 Hidden Layer 的神经网络。\n面积和房价的线性拟合 多个房屋特征的神经网络 有时候特征向量与因变量并不是简单的线性关系，例如我们给的不是面积，而是房子的长宽，那么将会存在特征之间相乘的计算逻辑。或者有时候特征向量之间可能也存在关系，例如加上时间维度来预测未来的房价，那么其他特征带来的效果可能会跟着时间变化发生变化。如果我们在各个线性回归的关系上再次建立回归模型，那么工作量和计算量将会特别高，但多层的神经网络来处理这个问题就非常方便。\n神经网络本身是由许多节点组成的，每个节点的输入输出都可以认为是一次线性转化，因此可以认为神经网络会将问题分为多个子问题来处理，不同纬度的问题会被分到各个层级，同一纬度的子问题会被分到各个神经元。每个神经元利用线性回归来对输入数据的特征进行线性转换（这个过程也称为特征提取），将子问题分析结果反馈给下一层级（父问题）继续处理。\n没有 Hidden Layer 的神经网络只能用于表示线性回归函数，但多层的网络则可以在线性回归上建立更高纬度的模型。下面是 The Number of Hidden Layers 中总结的常见层数体系结构的功能：\nThe Number of Hidden Layers Feature 没有隐含层 仅能够表示线性可分函数或决策 隐含层数=1 可以拟合任何“包含从一个有限空间到另一个有限空间的连续映射”的函数 隐含层数=2 搭配适当的激活函数可以表示任意精度的任意决策边界，并且可以拟合任何精度的任何平滑映射 隐含层数\u003e2 多出来的隐藏层可以学习复杂的描述 层数越深，理论上拟合函数的能力增强，效果按理说会更好，但是实际上更深的层数可能会带来过拟合的问题，同时也会增加训练难度，使模型难以收敛。\n这时我们可以通过增加隐含层层数来增加模型的数据处理维度，以处理更加复杂的房价关系问题。\n在分类问题中，线性回归也可以用来作为判别函数，例如有一个 𝑓(𝑥;𝑤)= 𝒘T𝒙 + 𝚋 将特征空间中满足线性判别函数 y=0 的点组成一个决策边界，将特征空间分为两个区域，每个区域对应一个类别，下图是二分类的例子。\n当分类问题是类别数C大于2的多分类问题时，一般需要通过“一对其余”、“一对一”或“argmax”方式来设计多个线性判别函数。\n激活函数 如果神经网络每一层都只是接收上一层输入函数的线性变化，那么无论神经网络模型多复杂最终输出也是线性组合，纯粹的线性组合并不能解决更复杂的问题。\n例如我们知道房价不会为负数，也就是房价预估的输出实际上不是一个线性回归问题，上面的线性回归函数只是满足了我们对数据的拟合，函数计算值并不能作为最后的结果输出。因此在人工神经元进行线性转换后，需要再作用于另一个函数，也就是激活函数。\n激活函数可以分为线性激活函数（例如 𝑓(x)=x），以及非线性激活函数。由于神经元对输入数据的处理本身就是进行线性转换，因此为了增加网络的表达能力，激活函数一般使用的是非线性激活函数。\n激活函数一般需要具备以下几点特性：\n连续并且可导出（允许少数点上不可导），可导的激活函数 可以直接利用数值优化的方法来学习网络参数 激活函数及其导函数要尽可能简单，有利于提高网络计算效率 激活函数的导函数的值域要在一个合适的区间，区间大小会影响训练的效率和稳定性 常用的激活函数有 ReLU、Sigmoid、Tanh、Softmax 等。\n以线性修正函数（Rectified Linear Unit, ReLU）为例，当 x 大于 0 时，输出 x 的值，当 x 小于等于 0 时，输出 0，其表达式如下\nimport matplotlib.pyplot as plt import numpy as np rule = lambda z: np.maximum(0, z) start = -10 stop = 10 step = 0.1 num = (stop - start) / step x = np.linspace(start, stop, int(num)) y = rule(x) plt.plot(x, y, label='ReLU') plt.grid(True) plt.legend() plt.show() 例如当我们把 ReLU 应用到房屋预测模型的神经元中，那么将会得到下面的逻辑回归模型。\n逻辑回归 逻辑回归（Logistic Regression）是线形回归中使用 Sigmoid 作为激活函数的线性模型。常用与处理二分类问题。线形回归在二分类中只能拟合出一个决策边界，分类问题最终需要根据这个决策边界来得到最终类别结果。\n二分类中使用逻辑回归的表达式为\nimport matplotlib.pyplot as plt import numpy as np sigmoid = lambda z: 1 / (1 + np.exp(-z)) start = -10 stop = 10 step = 0.01 num = (stop - start) / step x = np.linspace(start, stop, int(num)) y = sigmoid(x) plt.plot(x, y, label='Sigmoid') plt.grid(True) plt.legend() plt.show() 为什么选择 Sigmoid 作为逻辑回归的激活函数可以看【参考5】的文章\nLogistic 回归可以看作预测值为“标签的对数几率”的线性回归模型．因此， Logistic回归也称为对数几率回归（Logit Regression）。\n一维数据二分类（图来自神经网络与深度学习）\n二维数据二分类\n多分类问题中使用的是 softmax 函数\n前馈神经网络 前馈神经网络模型是神经网络中最简单最基础的模型，本文许多知识点都是以前馈神经网络为基础来讲解。前馈神经网络每层的每个节点，都会和上一层/下一层的所有节点建立连接，该层为全连接层，所有层次都是全连接层的网络也称为全连接网络。\n实际上网络可以根据不同特征之间的关系来建立对应的连接，并不需要全部连接在一起，但这会增加许多模型结构建立的人为工作量。\n全连接网络在训练的过程中，那些不需要的连接权重会被置 0（或接近0），此时可以认为两个节点之间是没有连接关系的，这样我们就不需要再去关心那些节点应该连接起来了。\n损失函数 对于单个神经元来说，需要确定的参数就是权重 _ω_j 的值，所有神经元之间连接的权重组成权重矩阵，权重矩阵大幅决定了模型最终输出结果的准确性，在机器学习中，会使用损失函数来评估一个模型的好坏。\n损失函数（误差函数、代价函数，Cost Function / Loss function）用于衡量模型的预测值 𝑓(x) 和预期值 y 的不一致程度，它是一个非负实值函数，损失函数在测试数据上输出的值越小，可以认为模型的准确度越高。\n模型训练最终目的就是得到最佳的权重，使损失函数在测试数据上的值最小。\n损失函数有多种，例如线性回归经常使用的平方损失。\n对于线性回归函数，我们可以使用最小二乘法则来得到各个权重最佳值，最小二乘法的公式如下，其中 𝜃 代表权重矩阵，通过损失函数对 𝜃 求导取 0 从而得到最优 𝜃。\n使用最小二乘法则的使用需要考虑下面两个问题：\n需要计算逆矩阵，时间复杂度为 O(n^3)，当特征数变多时将会非常耗时 需要 XTX 可逆，当训练样本数小于特征数，或者特征之间存在线性关系，那么 XTX 将不可逆 最小二乘法只适用于线性模型 对于神经网络而言，由于上面的问题，一般并不推荐使用最小二乘法则来计算神经网络的权重矩阵。 我们可以将权重矩阵的计算问题，转化为损失函数的优化问题，使用最优化方法来优化损失函数的输出，得到目标权重矩阵。\n梯度下降 梯度下降算法（Gradient Descent）属于最优化方法的一种，由于它的时间复杂度和初始要求都比较低，相对其他最优化方法，更加适合神经网络这种特征维度大的场景。\n梯度下降算法的思路是，先给所有权重一个初始值，每次迭代时更新权重，使损失函数的值往期望的方向变化。 就像一个人在下山，会根据当前位置，往低的地方迈出一步，最终到局部最低点。\nfrom matplotlib import pyplot, cm import numpy as np fig = pyplot.figure() axes = pyplot.axes(projection='3d') xx = np.arange(-10,10,0.1) yy = np.arange(-10,10,0.1) X, Y = np.meshgrid(xx, yy) Z = X**2+Y**2+10 axes.plot_surface(X,Y,Z,alpha=0.9,cmap=cm.coolwarm) axes.contour(X,Y,Z,zdir='z', offset=-5,cmap=\"rainbow\") axes.set_xlabel('w1') axes.set_xlim(-9, 9) axes.set_ylabel('w2') axes.set_ylim(-9, 9) axes.set_zlabel('cost') axes.set_zlim(-5, 200) pyplot.show() 梯度下降算法中，每次迭代更新时所有权重要一起更新，从而达到整体位置的向下偏移，每个权重的更新公式如下，其中：\n:=号为赋值符号 𝜔𝑗 为进行更新的权重 𝛼 为学习速率，该值应当大于 0 后面那部分为损失函数的偏导数，也就是损失函数在 𝜔𝑗 方向的斜率 当损失函数在 𝜔𝑗 方向的斜率是负数时， 𝜔𝑗 减去一个负数， 𝜔𝑗 变大 当损失函数在 𝜔𝑗 方向的斜率是正数时， 𝜔𝑗 减去一个正数， 𝜔𝑗 变小 无论是偏导数是正数还是负数，𝜔𝑗 损失函数总会向着 0 值的方向变化，最终接近 𝜔𝑗 的局部最优解。\nfrom matplotlib import pyplot import numpy as np fig = pyplot.figure() X = np.arange(-10,10,0.1) Y = X**2 pyplot.plot(X, Y) pyplot.xlabel(\"w\") pyplot.ylabel(\"cost\") pyplot.ylim(0, 100) pyplot.ylim(0, 100) pyplot.xticks([]) pyplot.yticks([]) pyplot.show() 学习速率 学习速率 𝛼 决定了梯度下降每次迭代时，权重 𝜔𝑗 的更新距离，也可以称为单次训练的步长。\n当 𝛼 太小时，每次迭代带来的下降值也会非常小，这意味着需要迭代更多次数才能到达局部最优解。\n当 𝛼 太大时，每次迭代权重 𝜔𝑗 的变化也会很大，这可能出现两个问题：\n损失函数导数变小，但接近最小值时发生震荡，无法收敛 损失函数导数变大，学习速率不变，结果发散 为了解决学习速率的问题，目前也有很多对学习速率进行改良的梯度下降算法：\nAdaGrad：每次迭代时，学习速率根据梯度平方积累值的增加逐渐衰减 RMSprop：AdaGrad 优化版，在衰减过程中进行加权移动 Monmentum：基于物理加速度和阻力的思路，更新参数时加上一个冲量，当冲量和梯度方向相同时冲量会增加，相反时冲量会减少 Adam：自适应矩估计，Monmentum + RMSprop 的结合体 非凸函数 上面提到的权重矩阵求解都是指局部最优解。拟合函数并不总是像上面的图一样，是一个弓形的函数（凸函数），当集合里面任意两个点的连线都在落集合里面，否则则认为是非凸问题。\n机器学习中许多问题属于非凸问题，非凸优化问题可能存在多个局部最优解，因此使用梯度下降算法得到的不一定是全局最优解，这与初始权重使损失函数值落在哪一点有关系。\n小批量梯度下降 批量梯度下降（Batch Gradient Descent, BGD）\nBGD 是训练时，采用整个样本来优化算法。BGD 虽然迭代次数能相对比较少，但一次迭代都要遍历所有样本，需要大量的时间，并且更新在所有样本遍历完才发生，在全连接网络中多余的参数更新也会被计算进去。\n随机梯度下降（Stochastic Gradient Descent, SGD）\n每次迭代使用一个样本来更新参数。SGD 相比 BGD 会多出噪声，提高了泛化误差，但学习过程较慢，遇到局部极小或鞍点容易卡在梯度 0 的地方。现在的 SGD** **更多指的是小批量随机梯度下降，下文也一样。\n小批量梯度下降（Min-batch Gradient Descent, MBGD）\nBGD 和 SGD 的结合，即每次迭代从打乱的训练集中随机抽取一小批数据样本来更新。\n反向传播算法 由于神经网络模型的误差计算在输出层，因此使用梯度下降算法来训练时，隐含层没办法直接获得误差来更新参数。这时可以通过反向传播算法来将误差传递给上一层来更新权重。\n反向传播算法（Back Propagation, BP）是一个和其他最优化方法结合更新神经网络参数的方法，其的思路是，当前节点计算出来的结果与预期值的误差，和上一层节点的输入有关，上层各节点的输入对误差带来的影响应该是不同的，因此需要合理地将误差分配给上层的神经元，控制上层权重变化比例来更快的降低代价。\n反向传播算法和梯度下降结合使用时，可以直接计算权重相对于最终输出（损失）的梯度，不用计算隐藏层值相对于权重变化的导数。\n反向传播的公式推导并不容易，我们先直接记下公式\n使用反向传播算法的神经网络训练流程如下：\n从训练集里随机获取一个/批训练样本 前馈计算每层的净输入和激活值，直到最后一层 使用公式 1 计算输出值和预期值的误差 使用公式 2 反向传播计算每一层的误差值 使用公式 3 和 4 更新权重参数和偏置量 回到 1 进行下一次迭代 到这里，一个基础的前馈神经网络模型的输出和训练流程都讲到了，下面可以开始动手写代码了。\n代码实践 本次代码实践使用手写图片识别作为的例子，因为有开放的数据集，并且手写时模型结构也可以定义的比较简单。\n手写图片的数据集可以从 MNIST 官网下载，为了方便，我们使用 TrochVision 来获取 MNIST 数据集，它会自动帮我们下载 MNIST 数据集并解压，获取时也会提供了对应的数据转换。\n这里会再使用一个 numpy 的库，这个库可以帮助我们完成一些复杂的矩阵计算。\nimport numpy as np import torchvision torchvision.datasets.MNIST(root='data/', train=True, download=True) 首先我们先来看看 MNIST 数据长什么样。\nMNIST 数据集为 0～9 的手写数字图片，有 60000 张训练样本，还有 10000 张测试样本，每张图片的分辨率为 28 * 28。 我们可以定义 28 * 28 = 784 个节点的输入层，使用 10 个节点的输出层，每个节点输出代表 0～9 各数字的决策值，输出 1 时代表为该数字。\n定义模型结构 手写图片识别属于分类问题，因此我们可以采用逻辑回归，先定义 Sigmoid 激活函数以及其导数形式\nsigmoid = lambda z: 1 / (1 + np.exp(-z)) derivative_sigmoid = lambda z: sigmoid(z) * (1 - sigmoid(z)) 图片特征提取属于高维度的回归问题，二维矩阵 + 矩阵特征转换，因此可以定义一个 3 层的神经网络（1层输入，2层隐含，1层输出）。训练方式使用 MBGD 算法 + BP 算法。因此我们可以简单地定义下面的的模型结构。\nclass NeuralNetwork(object): def __init__(self, l0, l1, l2, l3, batch_size=6): \"\"\" 初始化神经网络 :param l0: 输入层节点数 :param l1: 隐含层 l1 节点数 :param l2: 隐含层 l2 节点数 :param l3: 输出层节点数量 :param batch_size: 单次训练批次数据量 \"\"\" self.lr = 0.5 # 学习率 self.batch_size = batch_size # 各层权重与偏置量 self.w1 = np.random.randn(l0, l1) * 0.01 self.b1 = np.random.randn(l1) * 0.01 self.w2 = np.random.randn(l1, l2) * 0.01 self.b2 = np.random.randn(l2) * 0.01 self.w3 = np.random.randn(l2, l3) * 0.01 self.b3 = np.random.randn(l3) * 0.01 算法实现 定义了模型的结构，接下来就是前馈传播和反向传播算法的实现了，由于反向传播算法需要使用到各层在前馈时的净输入和激活值，因此前馈方法会将这些数据返回，用于反向传播。\n只需要使用代码将数据代入上面提到的公式，因此实现起来很简单。\n# in NeuralNetwork def forward(self, x): \"\"\" 向前传播推导结果 :param x: 输入的 [784] 向量矩阵 :return: 输出各层的净输入和激活值 \"\"\" z1 = np.dot(x, self.w1) + self.b1 o1 = sigmoid(z1) z2 = np.dot(o1, self.w2) + self.b2 o2 = sigmoid(z2) z3 = np.dot(o2, self.w3) + self.b3 o3 = sigmoid(z3) return z1, o1, z2, o2, z3, o3 # in NeuralNetwork def backward(self, x, z1, o1, z2, o2, err3): \"\"\" 反向传播更新权重 \"\"\" dot_w3 = np.dot(o2.T, err3) / self.batch_size dot_b3 = np.sum(err3, axis=0) / self.batch_size err2 = np.dot(err3, self.w3.T) * derivative_sigmoid(z2) dot_w2 = np.dot(o1.T, err2) / self.batch_size dot_b2 = np.sum(err2, axis=0) / self.batch_size err1 = np.dot(err2, self.w2.T) * derivative_sigmoid(z1) dot_w1 = np.dot(x.T, err1) / self.batch_size dot_b1 = np.sum(err1, axis=0) / self.batch_size self.w3 -= self.lr * dot_w3 self.b3 -= self.lr * dot_b3 self.w2 -= self.lr * dot_w2 self.b2 -= self.lr * dot_b2 self.w1 -= self.lr * dot_w1 self.b1 -= self.lr * dot_b1 训练与测试 最后编写训练和测试的方法，测试时使用完成训练的模型。由于我们采用小批次梯度，因此取数据时需要按照每个批次的数据量来取。另外一次样本的遍历可能不足以让模型得到很好的效果，因此我们可以进行多次全样本的训练。训练时我们也可以计算一下当前批次的损失，来观察模型的拟合情况。\n测试时采用一次性计算，得到所有测试数据的结果矩阵，对预测结果获取最大值索引，也就是单个样本预测结果为 1 的位置，该位置为样本的预测结果数值，最后计算样本预测值和预期值的匹配数量，来得到准确率。\ndef train(nn, data, targets): for cou in range(10): for i in range(0, 60000, nn.batch_size): x = data[i:i + nn.batch_size] y = targets[i:i + nn.batch_size] z1, o1, z2, o2, z3, o3 = nn.forward(x) err3 = (o3 - y) * derivative_sigmoid(z3) loss = np.sum((o3 - y) * (o3 - y)) / nn.batch_size print(\"cou:\" + str(cou) + \", err:\" + str(loss)) nn.backward(x, z1, o1, z2, o2, err3) def test(nn, data, targets): _, _, _, _, _, o3 = nn.forward(data) result = np.argmax(o3, axis=1) precision = np.sum(result == targets) / 10000 print(\"Precision:\", precision) 数据预处理 在获取数据时，我们需要先将图片二维的像素数据平铺成一维矩阵，将对应的数字标签 0～9 转换成 1 维矩阵的输出，例如 3 转换为 [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]。由于测试数据的标签不需要参与反向传播，我们不做矩阵转换，这样可以方便我们对预测结果做计算。\ndef target_matrix(targets): \"\"\" 数字标签转换 :param targets: 对于的数字标签矩阵 :return: \"\"\" num = len(targets) result = np.zeros((num, 10)) for i in range(num): result[i][targets[i]] = 1 return result # 训练数据 def load_train_data(): train_data = torchvision.datasets.MNIST(root='data/', train=True, download=True) # Numpy 矩阵转换 train_data.data = train_data.data.numpy() # [60000,28,28] train_data.targets = train_data.targets.numpy() # [60000] # 输入向量处理，将二维数据平铺 train_data.data = train_data.data.reshape(60000, 28 * 28) / 255. # (60000, 784) # 标签转换 train_data.targets = target_matrix(train_data.targets) # (60000, 10) return train_data # 测试数据 def load_test_data(): test_data = torchvision.datasets.MNIST(root='data/', train=False) test_data.data = test_data.data.numpy() # [10000,28,28] test_data.targets = test_data.targets.numpy() # [10000] test_data.data = test_data.data.reshape(10000, 28 * 28) / 255. # (10000, 784) return test_data 最后把上面的步骤组织起来\ndef demo(): nn = NeuralNetwork(784, 200, 30, 10) train_data = load_train_data() train(nn, train_data.data, train_data.targets) test_data = load_test_data() test(nn, test_data.data, test_data.targets) demo() 最终测试结果 Precision: 0.9686，即准确率有 96%。\n代码地址：https://github.com/korilin/neural_network_tech_sharing\n其他网络模型设计 除了前馈神经网络外，神经网络的节点类型还有很多，不同模型的训练/处理消耗的资源，以及应用场景也不一样。\n深度神经网络 在机器学习中有一个深度学习话题，在神经网络中，深度学习体现在网络隐含层数量，层数多的网络称为深度神经网络（DNN），像深度残差学习网络最多能有 152 层，但随着网络增加训练难度也非常大。\n卷积神经网络 卷积神经网络（CNN）也是比较常用的网络结构，CNN 在图片识别上有非常理想的效果，许多图片、视频的识别/分类模型都采用了 CNN 的思想。\nCNN 的核心为卷积核，在图像中有着颜色通道，局部特征等特有特性，这些特征的特点来自相邻像素的关联性，通常这些特征在局部或整体上会给人带来不同的感受。因此出现了采用卷积来提取局部特征的方式，在数学算子上卷积是通过两个函数生成第三个函数，表征函数 f 与经过翻转和平移的 g 的乘积函数所围成的曲边梯形的面积。函数 g 在卷积神经网络中也称为滤波器。\n循环神经网络 在普通神经网络和 CNN 中，每层神经元的信号只能向上层传播，但这样对时间序列上的变化进行建模会比较困难，循环神经网络（RNN）就是为了适应这种需求出现的。 RNN 中的神经元的输出可以在下一个时间戳中直接作用到自身，也就是每个神经元的输入，出了上一层的输入外，还结合了神经元自身的上一次输入，用图表示如下 生成对抗网络 生成对抗网络（GAN）属于生成模型的一种，由一个生成网络和一个判别网络组成。 生成网络模仿真实样本生成假数据来给判别网络识别，判别网络则识别输入数据为真实样本还是生成网络生成的假数据。两个网络之间通过相互对抗和参数调整，最终使得判别网络无法判断生成网络的输出是否为真实数据。 GAN 是非监督式学习的一种，但实际上在半监督、强化学习中也有效果。 GAN 还有一个变体 DCGAN，由于 CNN 中卷积核对图片特征的提取具有非常好的效果，因此 DCGAN 中使用了反卷积来对特征进行反推，来对特征进行更好转换输出。\n参考 机器学习算法（一）：逻辑回归模型（Logistic Regression, LR）_意念回复的博客-CSDN博客 神经网络与深度学习 The Number of Hidden Layers 深度学习笔记：如何理解激活函数？（附常用激活函数） 机器学习之Logistic回归激活函数为什么是Sigmoid？_logistic回归的激活函数_MuBaicao的博客-CSDN博客 常见的几种最优化方法（梯度下降法、牛顿法、拟牛顿法、共轭梯度法等） - 蓝鲸王子 - 博客园 卷积神经网络中二维卷积核与三维卷积核有什么区别？_3d卷积和2d卷积区别_意念回复的博客-CSDN博客 生成模型（Generative）和判别模型（Discriminative）_生成模型和判别模型_意念回复的博客-CSDN博客 CNN（卷积神经网络）、RNN（循环神经网络）、DNN（深度神经网络）的内部网络结构有什么区别？ - 知乎 ","wordCount":"1069","inLanguage":"en","datePublished":"2023-04-13T00:00:00Z","dateModified":"2023-04-13T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"http://answerkobe.github.io/posts/basic-theory-and-simple-practice-of-neural-network/"},"publisher":{"@type":"Organization","name":"Iverson's blog","logo":{"@type":"ImageObject","url":"http://answerkobe.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://answerkobe.github.io/ accesskey=h title="Iverson's blog (Alt + H)">Iverson's blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://answerkobe.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=http://answerkobe.github.io/tags title=Tags><span>Tags</span></a></li><li><a href=http://answerkobe.github.io/profile title=Profile-X><span>Profile-X</span></a></li><li><a href=http://answerkobe.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://answerkobe.github.io/>Home</a>&nbsp;»&nbsp;<a href=http://answerkobe.github.io/posts/>Posts</a></div><h1 class=post-title>神经网络基础理论与简单实践</h1><div class=post-meta><span title='2023-04-13 00:00:00 +0000 UTC'>April 13, 2023</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0machine-learning-ml aria-label="机器学习（Machine Learning, ML）">机器学习（Machine Learning, ML）</a></li><li><a href=#%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%bb%93%e6%9e%84 aria-label=神经网络结构>神经网络结构</a><ul><li><a href=#%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92 aria-label=线性回归>线性回归</a></li><li><a href=#%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0 aria-label=激活函数>激活函数</a></li><li><a href=#%e9%80%bb%e8%be%91%e5%9b%9e%e5%bd%92 aria-label=逻辑回归>逻辑回归</a></li><li><a href=#%e5%89%8d%e9%a6%88%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label=前馈神经网络>前馈神经网络</a></li></ul></li><li><a href=#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0 aria-label=损失函数>损失函数</a></li><li><a href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d aria-label=梯度下降>梯度下降</a><ul><li><a href=#%e5%ad%a6%e4%b9%a0%e9%80%9f%e7%8e%87 aria-label=学习速率>学习速率</a></li><li><a href=#%e9%9d%9e%e5%87%b8%e5%87%bd%e6%95%b0 aria-label=非凸函数>非凸函数</a></li><li><a href=#%e5%b0%8f%e6%89%b9%e9%87%8f%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d aria-label=小批量梯度下降>小批量梯度下降</a></li></ul></li><li><a href=#%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95 aria-label=反向传播算法>反向传播算法</a></li><li><a href=#%e4%bb%a3%e7%a0%81%e5%ae%9e%e8%b7%b5 aria-label=代码实践>代码实践</a><ul><li><a href=#%e5%ae%9a%e4%b9%89%e6%a8%a1%e5%9e%8b%e7%bb%93%e6%9e%84 aria-label=定义模型结构>定义模型结构</a></li><li><a href=#%e7%ae%97%e6%b3%95%e5%ae%9e%e7%8e%b0 aria-label=算法实现>算法实现</a></li><li><a href=#%e8%ae%ad%e7%bb%83%e4%b8%8e%e6%b5%8b%e8%af%95 aria-label=训练与测试>训练与测试</a></li><li><a href=#%e6%95%b0%e6%8d%ae%e9%a2%84%e5%a4%84%e7%90%86 aria-label=数据预处理>数据预处理</a></li></ul></li><li><a href=#%e5%85%b6%e4%bb%96%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%9e%8b%e8%ae%be%e8%ae%a1 aria-label=其他网络模型设计>其他网络模型设计</a><ul><li><a href=#%e6%b7%b1%e5%ba%a6%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label=深度神经网络>深度神经网络</a></li><li><a href=#%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label=卷积神经网络>卷积神经网络</a></li><li><a href=#%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label=循环神经网络>循环神经网络</a></li><li><a href=#%e7%94%9f%e6%88%90%e5%af%b9%e6%8a%97%e7%bd%91%e7%bb%9c aria-label=生成对抗网络>生成对抗网络</a></li></ul></li><li><a href=#%e5%8f%82%e8%80%83 aria-label=参考>参考</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=机器学习machine-learning-ml>机器学习（Machine Learning, ML）<a hidden class=anchor aria-hidden=true href=#机器学习machine-learning-ml>#</a></h2><p>在了解神经网络之前，我们需要先知道机器学习和神经网络之间的关系。</p><p>机器学习是人工智能中的一个分支， 主要研究如何让计算机模拟人类的学习行为来获得知识和技能，在实践上就是通过让泛型算法（Generic Algorithm，和 C/C++ 中的泛型算法不是一个东西）自己在输入的数据上建立逻辑模型，并通过该模型来得到目标结论。</p><p><img loading=lazy src=./image_1.png alt=image_1.png></p><p>泛型算法能处理的问题取决于我们所定义好的输入/输出的数据格式，而处理问题的条件和逻辑，则由泛型算法通过对输入数据进行分析学习构建处理逻辑参数，构建后的逻辑也能适应非预设数据的处理。神经网络属于机器学习泛型算法中的一种实现方案。</p><blockquote><ul><li>根据泛型算法的学习方式，也可以将机器学习分为监督式学习、弱监督学习、半监督学习、非监督式学习、迁移学习、强化学习等。</li><li>人工智能中许多维度的分类都存在交叉关系，因此通常不会为任何一个方法论进行确切的归类。</li></ul></blockquote><h2 id=神经网络结构>神经网络结构<a hidden class=anchor aria-hidden=true href=#神经网络结构>#</a></h2><p>【概念】机器学习中的神经网络模型是一种模仿生物神经网络结构和功能的模型，因此也被称为人工神经网络或类神经网络。人工神经网络由多个人工神经元（处理单元）以及传递信号的链接形成拓扑结构，由于泛型算法能处理的问题取决于输入/输出的数据格式，因此神经网络基本会分为3个层：</p><ul><li>Input Layer：输入层，用于接收外界数据，节点数量根据输入的数据类型决定</li><li>Hidden Layer：隐含层，负责对输入层提供的数据进行信息处理、信息转化，通常这一层会有多个层次，每层会将处理结果向后面传递。</li><li>Output Layer：输出层，将隐含层提供的输出信息转化层最终结果，节点数量根据输出的结构类型决定</li></ul><p><img loading=lazy src=./image_2.png alt=image_2.png></p><p>【生物知识点复习】生物神经元通常具有多个树突，树突用于接收信号，接收的信号在细胞体内整合产生阶梯性生电，而轴突用于传递信号，当细胞体的电位影响达到一定的阈值，则代表这个神经元被激活，激活的神经元会产生神经冲动（电脉冲），通过轴突传导出去，轴突尾端有许多和其他神经元树突产生连接的突触，电信号会通过这些突触传递给其他神经元，突触在一次突触事件中产生的电位幅度与突触的强度有关。</p><p><img loading=lazy src=./image_3.png alt=image_3.png></p><p>人工神经元也是模拟生物神经元的结构和特性，下面是一个典型的人工神经元结构，以及人工神经元和生物神经元中各个行为的对照表。</p><p><img loading=lazy src=./image_4.png alt=image_4.png></p><table><thead><tr><th><strong>生物神经元</strong></th><th><strong>人工神经元</strong></th></tr></thead><tbody><tr><td>输入，即上一个神经元轴突传递过来的电信号</td><td>x𝑖 也就是下一个神经元的输出 y</td></tr><tr><td>突触强度</td><td>w𝑗 权重</td></tr><tr><td>树突接收到的电信号</td><td>x𝑖 * w𝑗</td></tr><tr><td>信号积累，阶段性生电</td><td>∑(x𝑖 * w𝑗)</td></tr><tr><td>神经元激活</td><td>𝑓(·) 激活函数</td></tr><tr><td>轴突电信号传递</td><td>y 也就是下一个神经元的输入 x𝑖</td></tr></tbody></table><h3 id=线性回归>线性回归<a hidden class=anchor aria-hidden=true href=#线性回归>#</a></h3><p>从上面的对照表可以看出，一个神经元对上一层的输入处理其实就是将各个输入值加权求和，本质上就是一个线性回归 <code>𝑓(𝑥;𝑤) = 𝑤1𝑥1 + 𝑤2𝑥2 + ⋯ + 𝑤D𝑥D + 𝚋</code>。线性回归模型是机器学习中最基础最广泛的模型，主要用于分析自变量和因变量之间的关系。
为什么使用的是线性回归呢？
线性回归可以用来描述自变量和因变量之间的关系，机器学习中大部分问题都是分析数据里特征的关系来建立模型，因此在机器学习中很多问题都可以转换为线性回归问题来处理。
例如我们有如下左图关于面积和房价关系的数据，那么可以用一个一元线性回归模型来拟合这些数据，从而得到一个可以根据面积来预估房价的模型。当房屋特征变多时，也可以根据回归函数的参数建立一个没有 Hidden Layer 的神经网络。</p><table><thead><tr><th>面积和房价的线性拟合</th><th>多个房屋特征的神经网络</th></tr></thead><tbody><tr><td><img loading=lazy src=./image_5.png alt=image_5.png></td><td><img loading=lazy src=./image_6.png alt=image_6.png></td></tr></tbody></table><p>有时候特征向量与因变量并不是简单的线性关系，例如我们给的不是面积，而是房子的长宽，那么将会存在特征之间相乘的计算逻辑。或者有时候特征向量之间可能也存在关系，例如加上时间维度来预测未来的房价，那么其他特征带来的效果可能会跟着时间变化发生变化。如果我们在各个线性回归的关系上再次建立回归模型，那么工作量和计算量将会特别高，但多层的神经网络来处理这个问题就非常方便。</p><p>神经网络本身是由许多节点组成的，每个节点的输入输出都可以认为是一次线性转化，因此可以认为神经网络会将问题分为多个子问题来处理，不同纬度的问题会被分到各个层级，同一纬度的子问题会被分到各个神经元。每个神经元利用线性回归来对输入数据的特征进行线性转换（这个过程也称为特征提取），将子问题分析结果反馈给下一层级（父问题）继续处理。</p><p>没有 Hidden Layer 的神经网络只能用于表示线性回归函数，但多层的网络则可以在线性回归上建立更高纬度的模型。下面是 <a href=https://www.heatonresearch.com/2017/06/01/hidden-layers.html>The Number of Hidden Layers</a> 中总结的常见层数体系结构的功能：</p><table><thead><tr><th><strong>The Number of Hidden Layers</strong></th><th><strong>Feature</strong></th></tr></thead><tbody><tr><td>没有隐含层</td><td>仅能够表示线性可分函数或决策</td></tr><tr><td>隐含层数=1</td><td>可以拟合任何“包含从一个有限空间到另一个有限空间的连续映射”的函数</td></tr><tr><td>隐含层数=2</td><td>搭配适当的激活函数可以表示任意精度的任意决策边界，并且可以拟合任何精度的任何平滑映射</td></tr><tr><td>隐含层数>2</td><td>多出来的隐藏层可以学习复杂的描述</td></tr></tbody></table><blockquote><p>层数越深，理论上拟合函数的能力增强，效果按理说会更好，但是实际上更深的层数可能会带来过拟合的问题，同时也会增加训练难度，使模型难以收敛。</p></blockquote><p>这时我们可以通过增加隐含层层数来增加模型的数据处理维度，以处理更加复杂的房价关系问题。</p><p><img loading=lazy src=./image_7.png alt=image_7.png></p><p>在分类问题中，线性回归也可以用来作为判别函数，例如有一个 <code>𝑓(𝑥;𝑤)= 𝒘T𝒙 + 𝚋</code> 将特征空间中满足线性判别函数 <code>y=0</code> 的点组成一个决策边界，将特征空间分为两个区域，每个区域对应一个类别，下图是二分类的例子。</p><p><img loading=lazy src=./image_8.png alt=image_8.png></p><p>当分类问题是类别数C大于2的多分类问题时，一般需要通过“一对其余”、“一对一”或“argmax”方式来设计多个线性判别函数。</p><p><img loading=lazy src=./image_9.png alt=image_9.png></p><h3 id=激活函数>激活函数<a hidden class=anchor aria-hidden=true href=#激活函数>#</a></h3><p>如果神经网络每一层都只是接收上一层输入函数的线性变化，那么无论神经网络模型多复杂最终输出也是线性组合，纯粹的线性组合并不能解决更复杂的问题。</p><p>例如我们知道房价不会为负数，也就是房价预估的输出实际上不是一个线性回归问题，上面的线性回归函数只是满足了我们对数据的拟合，函数计算值并不能作为最后的结果输出。因此在人工神经元进行线性转换后，需要再作用于另一个函数，也就是激活函数。</p><p>激活函数可以分为线性激活函数（例如 <code>𝑓(x)=x</code>），以及非线性激活函数。由于神经元对输入数据的处理本身就是进行线性转换，因此为了增加网络的表达能力，激活函数一般使用的是非线性激活函数。</p><p>激活函数一般需要具备以下几点特性：</p><ul><li>连续并且可导出（允许少数点上不可导），可导的激活函数 可以直接利用数值优化的方法来学习网络参数</li><li>激活函数及其导函数要尽可能简单，有利于提高网络计算效率</li><li>激活函数的导函数的值域要在一个合适的区间，区间大小会影响训练的效率和稳定性</li></ul><p>常用的激活函数有 ReLU、Sigmoid、Tanh、Softmax 等。</p><p>以线性修正函数（Rectified Linear Unit, ReLU）为例，当 x 大于 0 时，输出 x 的值，当 x 小于等于 0 时，输出 0，其表达式如下</p><p><img loading=lazy src=./image_10.png alt=image_10.png>
<img loading=lazy src=./image_11.png alt=image_11.png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rule <span style=color:#f92672>=</span> <span style=color:#66d9ef>lambda</span> z: np<span style=color:#f92672>.</span>maximum(<span style=color:#ae81ff>0</span>, z)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>stop <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>step <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>num <span style=color:#f92672>=</span> (stop <span style=color:#f92672>-</span> start) <span style=color:#f92672>/</span> step
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(start, stop, int(num))
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> rule(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(x, y, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;ReLU&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>grid(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p><img loading=lazy src=./image_12.png alt=image_12.png></p><p>例如当我们把 ReLU 应用到房屋预测模型的神经元中，那么将会得到下面的逻辑回归模型。</p><p><img loading=lazy src=./image_13.png alt=image_13.png></p><h3 id=逻辑回归>逻辑回归<a hidden class=anchor aria-hidden=true href=#逻辑回归>#</a></h3><p>逻辑回归（Logistic Regression）是线形回归中使用 Sigmoid 作为激活函数的线性模型。常用与处理二分类问题。线形回归在二分类中只能拟合出一个决策边界，分类问题最终需要根据这个决策边界来得到最终类别结果。</p><p>二分类中使用逻辑回归的表达式为</p><p><img loading=lazy src=./image_14.png alt=image_14.png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sigmoid <span style=color:#f92672>=</span> <span style=color:#66d9ef>lambda</span> z: <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>exp(<span style=color:#f92672>-</span>z))
</span></span><span style=display:flex><span>start <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>stop <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>step <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.01</span>
</span></span><span style=display:flex><span>num <span style=color:#f92672>=</span> (stop <span style=color:#f92672>-</span> start) <span style=color:#f92672>/</span> step
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(start, stop, int(num))
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> sigmoid(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(x, y, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Sigmoid&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>grid(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p><img loading=lazy src=./image_15.png alt=image_15.png></p><p>为什么选择 Sigmoid 作为逻辑回归的激活函数可以看<a href=https://blog.csdn.net/woshicao11/article/details/81458972>【参考5】</a>的文章</p><p>Logistic 回归可以看作预测值为“标签的对数几率”的线性回归模型．因此， Logistic回归也称为对数几率回归（Logit Regression）。</p><p>一维数据二分类（图来自<a href=https://nndl.github.io/nndl-book.pdf>神经网络与深度学习</a>）</p><p><img loading=lazy src=./image_16.png alt=image_16.png></p><p>二维数据二分类</p><p><img loading=lazy src=./image_17.png alt=image_17.png></p><blockquote><p>多分类问题中使用的是 softmax 函数</p></blockquote><h3 id=前馈神经网络>前馈神经网络<a hidden class=anchor aria-hidden=true href=#前馈神经网络>#</a></h3><p>前馈神经网络模型是神经网络中最简单最基础的模型，本文许多知识点都是以前馈神经网络为基础来讲解。前馈神经网络每层的每个节点，都会和上一层/下一层的所有节点建立连接，该层为全连接层，所有层次都是全连接层的网络也称为全连接网络。</p><p>实际上网络可以根据不同特征之间的关系来建立对应的连接，并不需要全部连接在一起，但这会增加许多模型结构建立的人为工作量。</p><p>全连接网络在训练的过程中，那些不需要的连接权重会被置 0（或接近0），此时可以认为两个节点之间是没有连接关系的，这样我们就不需要再去关心那些节点应该连接起来了。</p><p><img loading=lazy src=./image_18.png alt=image_18.png></p><h2 id=损失函数>损失函数<a hidden class=anchor aria-hidden=true href=#损失函数>#</a></h2><p>对于单个神经元来说，需要确定的参数就是权重 _ω_j 的值，所有神经元之间连接的权重组成权重矩阵，权重矩阵大幅决定了模型最终输出结果的准确性，在机器学习中，会使用损失函数来评估一个模型的好坏。</p><p>损失函数（误差函数、代价函数，Cost Function / Loss function）用于衡量模型的预测值 𝑓(x) 和预期值 y 的不一致程度，它是一个非负实值函数，损失函数在测试数据上输出的值越小，可以认为模型的准确度越高。</p><p>模型训练最终目的就是得到最佳的权重，使损失函数在测试数据上的值最小。</p><p>损失函数有多种，例如线性回归经常使用的平方损失。</p><p><img loading=lazy src=./image_19.png alt=image_19.png></p><p>对于线性回归函数，我们可以使用<strong>最小二乘法则</strong>来得到各个权重最佳值，最小二乘法的公式如下，其中 𝜃 代表权重矩阵，通过损失函数对 𝜃 求导取 0 从而得到最优 𝜃。</p><p><img loading=lazy src=./image_20.png alt=image_20.png></p><p>使用最小二乘法则的使用需要考虑下面两个问题：</p><ul><li>需要计算逆矩阵，时间复杂度为 O(n^3)，当特征数变多时将会非常耗时</li><li>需要 XTX 可逆，当训练样本数小于特征数，或者特征之间存在线性关系，那么 XTX 将不可逆</li><li>最小二乘法只适用于线性模型</li></ul><p>对于神经网络而言，由于上面的问题，一般并不推荐使用最小二乘法则来计算神经网络的权重矩阵。
我们可以将权重矩阵的计算问题，转化为损失函数的优化问题，使用最优化方法来优化损失函数的输出，得到目标权重矩阵。</p><h2 id=梯度下降>梯度下降<a hidden class=anchor aria-hidden=true href=#梯度下降>#</a></h2><p>梯度下降算法（Gradient Descent）属于最优化方法的一种，由于它的时间复杂度和初始要求都比较低，相对其他最优化方法，更加适合神经网络这种特征维度大的场景。</p><p>梯度下降算法的思路是，先给所有权重一个初始值，每次迭代时更新权重，使损失函数的值往期望的方向变化。
就像一个人在下山，会根据当前位置，往低的地方迈出一步，最终到局部最低点。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> matplotlib <span style=color:#f92672>import</span> pyplot, cm
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig <span style=color:#f92672>=</span> pyplot<span style=color:#f92672>.</span>figure()
</span></span><span style=display:flex><span>axes <span style=color:#f92672>=</span> pyplot<span style=color:#f92672>.</span>axes(projection<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;3d&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>xx <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(<span style=color:#f92672>-</span><span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>0.1</span>)
</span></span><span style=display:flex><span>yy <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(<span style=color:#f92672>-</span><span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>0.1</span>)
</span></span><span style=display:flex><span>X, Y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>meshgrid(xx, yy)
</span></span><span style=display:flex><span>Z <span style=color:#f92672>=</span> X<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span><span style=color:#f92672>+</span>Y<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span><span style=color:#f92672>+</span><span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>axes<span style=color:#f92672>.</span>plot_surface(X,Y,Z,alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>,cmap<span style=color:#f92672>=</span>cm<span style=color:#f92672>.</span>coolwarm)
</span></span><span style=display:flex><span>axes<span style=color:#f92672>.</span>contour(X,Y,Z,zdir<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;z&#39;</span>, offset<span style=color:#f92672>=-</span><span style=color:#ae81ff>5</span>,cmap<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;rainbow&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>axes<span style=color:#f92672>.</span>set_xlabel(<span style=color:#e6db74>&#39;w1&#39;</span>)
</span></span><span style=display:flex><span>axes<span style=color:#f92672>.</span>set_xlim(<span style=color:#f92672>-</span><span style=color:#ae81ff>9</span>, <span style=color:#ae81ff>9</span>)
</span></span><span style=display:flex><span>axes<span style=color:#f92672>.</span>set_ylabel(<span style=color:#e6db74>&#39;w2&#39;</span>)
</span></span><span style=display:flex><span>axes<span style=color:#f92672>.</span>set_ylim(<span style=color:#f92672>-</span><span style=color:#ae81ff>9</span>, <span style=color:#ae81ff>9</span>)
</span></span><span style=display:flex><span>axes<span style=color:#f92672>.</span>set_zlabel(<span style=color:#e6db74>&#39;cost&#39;</span>)
</span></span><span style=display:flex><span>axes<span style=color:#f92672>.</span>set_zlim(<span style=color:#f92672>-</span><span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>200</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pyplot<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p><img loading=lazy src=./image_21.png alt=image_21.png></p><p>梯度下降算法中，每次迭代更新时所有权重要一起更新，从而达到整体位置的向下偏移，每个权重的更新公式如下，其中：</p><ul><li><code>:=</code>号为赋值符号</li><li><code>𝜔𝑗</code> 为进行更新的权重</li><li><code>𝛼</code> 为学习速率，该值应当大于 0</li><li>后面那部分为损失函数的偏导数，也就是损失函数在 <code>𝜔𝑗</code> 方向的斜率</li></ul><p><img loading=lazy src=./image_22.png alt=image_22.png></p><ul><li>当损失函数在 <code>𝜔𝑗</code> 方向的斜率是负数时， <code>𝜔𝑗</code> 减去一个负数， <code>𝜔𝑗</code> 变大</li><li>当损失函数在 <code>𝜔𝑗</code> 方向的斜率是正数时， <code>𝜔𝑗</code> 减去一个正数， <code>𝜔𝑗</code> 变小</li></ul><p>无论是偏导数是正数还是负数，<code>𝜔𝑗</code> 损失函数总会向着 0 值的方向变化，最终接近 <code>𝜔𝑗</code> 的局部最优解。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> matplotlib <span style=color:#f92672>import</span> pyplot
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>fig <span style=color:#f92672>=</span> pyplot<span style=color:#f92672>.</span>figure()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(<span style=color:#f92672>-</span><span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>0.1</span>)
</span></span><span style=display:flex><span>Y <span style=color:#f92672>=</span> X<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pyplot<span style=color:#f92672>.</span>plot(X, Y)
</span></span><span style=display:flex><span>pyplot<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;w&#34;</span>)
</span></span><span style=display:flex><span>pyplot<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;cost&#34;</span>)
</span></span><span style=display:flex><span>pyplot<span style=color:#f92672>.</span>ylim(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>pyplot<span style=color:#f92672>.</span>ylim(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>pyplot<span style=color:#f92672>.</span>xticks([])
</span></span><span style=display:flex><span>pyplot<span style=color:#f92672>.</span>yticks([])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pyplot<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p><img loading=lazy src=./image_23.png alt=image_23.png></p><h3 id=学习速率>学习速率<a hidden class=anchor aria-hidden=true href=#学习速率>#</a></h3><p>学习速率 <code>𝛼</code> 决定了梯度下降每次迭代时，权重 <code>𝜔𝑗</code> 的更新距离，也可以称为单次训练的步长。</p><p>当 <code>𝛼</code> 太小时，每次迭代带来的下降值也会非常小，这意味着需要迭代更多次数才能到达局部最优解。</p><p>当 <code>𝛼</code> 太大时，每次迭代权重 <code>𝜔𝑗</code> 的变化也会很大，这可能出现两个问题：</p><ul><li>损失函数导数变小，但接近最小值时发生震荡，无法收敛</li><li>损失函数导数变大，学习速率不变，结果发散</li></ul><p><img loading=lazy src=./image_24.png alt=image_24.png></p><p>为了解决学习速率的问题，目前也有很多对学习速率进行改良的梯度下降算法：</p><ul><li>AdaGrad：每次迭代时，学习速率根据梯度平方积累值的增加逐渐衰减</li><li>RMSprop：AdaGrad 优化版，在衰减过程中进行加权移动</li><li>Monmentum：基于物理加速度和阻力的思路，更新参数时加上一个冲量，当冲量和梯度方向相同时冲量会增加，相反时冲量会减少</li><li>Adam：自适应矩估计，Monmentum + RMSprop 的结合体</li></ul><h3 id=非凸函数>非凸函数<a hidden class=anchor aria-hidden=true href=#非凸函数>#</a></h3><p>上面提到的权重矩阵求解都是指局部最优解。拟合函数并不总是像上面的图一样，是一个弓形的函数（凸函数），当集合里面任意两个点的连线都在落集合里面，否则则认为是非凸问题。</p><p><img loading=lazy src=./image_25.png alt=image_25.png></p><p>机器学习中许多问题属于非凸问题，非凸优化问题可能存在多个局部最优解，因此使用梯度下降算法得到的不一定是全局最优解，这与初始权重使损失函数值落在哪一点有关系。</p><p><img loading=lazy src=./image_26.png alt="Andrew Ng ML Course"></p><h3 id=小批量梯度下降>小批量梯度下降<a hidden class=anchor aria-hidden=true href=#小批量梯度下降>#</a></h3><ul><li><p><strong>批量梯度下降（Batch Gradient Descent, BGD）</strong></p><p>BGD 是训练时，采用整个样本来优化算法。BGD 虽然迭代次数能相对比较少，但一次迭代都要遍历所有样本，需要大量的时间，并且更新在所有样本遍历完才发生，在全连接网络中多余的参数更新也会被计算进去。</p></li><li><p><strong>随机梯度下降（Stochastic Gradient Descent, SGD）</strong></p><p>每次迭代使用一个样本来更新参数。SGD 相比 BGD 会多出噪声，提高了泛化误差，但学习过程较慢，遇到局部极小或鞍点容易卡在梯度 0 的地方。现在的 SGD** **更多指的是小批量随机梯度下降，下文也一样。</p></li><li><p><strong>小批量梯度下降（Min-batch Gradient Descent, MBGD）</strong></p><p>BGD 和 SGD 的结合，即每次迭代从打乱的训练集中随机抽取一小批数据样本来更新。</p></li></ul><h2 id=反向传播算法>反向传播算法<a hidden class=anchor aria-hidden=true href=#反向传播算法>#</a></h2><p>由于神经网络模型的误差计算在输出层，因此使用梯度下降算法来训练时，隐含层没办法直接获得误差来更新参数。这时可以通过反向传播算法来将误差传递给上一层来更新权重。</p><p>反向传播算法（Back Propagation, BP）是一个和其他最优化方法结合更新神经网络参数的方法，其的思路是，当前节点计算出来的结果与预期值的误差，和上一层节点的输入有关，上层各节点的输入对误差带来的影响应该是不同的，因此需要合理地将误差分配给上层的神经元，控制上层权重变化比例来更快的降低代价。</p><p><img loading=lazy src=./image_27.png alt=image_27.png></p><p>反向传播算法和梯度下降结合使用时，可以直接计算权重相对于最终输出（损失）的梯度，不用计算隐藏层值相对于权重变化的导数。</p><p>反向传播的公式推导并不容易，我们先直接记下公式</p><p><img loading=lazy src=./image_28.png alt=image_28.png></p><p>使用反向传播算法的神经网络训练流程如下：</p><ol><li>从训练集里随机获取一个/批训练样本</li><li>前馈计算每层的净输入和激活值，直到最后一层</li><li>使用公式 1 计算输出值和预期值的误差</li><li>使用公式 2 反向传播计算每一层的误差值</li><li>使用公式 3 和 4 更新权重参数和偏置量</li><li>回到 1 进行下一次迭代</li></ol><p>到这里，一个基础的前馈神经网络模型的输出和训练流程都讲到了，下面可以开始动手写代码了。</p><h2 id=代码实践>代码实践<a hidden class=anchor aria-hidden=true href=#代码实践>#</a></h2><p>本次代码实践使用手写图片识别作为的例子，因为有开放的数据集，并且手写时模型结构也可以定义的比较简单。</p><p>手写图片的数据集可以从 <a href=http://yann.lecun.com/exdb/mnist/>MNIST 官网</a>下载，为了方便，我们使用 TrochVision 来获取 MNIST 数据集，它会自动帮我们下载 MNIST 数据集并解压，获取时也会提供了对应的数据转换。</p><p>这里会再使用一个 numpy 的库，这个库可以帮助我们完成一些复杂的矩阵计算。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torchvision
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>torchvision<span style=color:#f92672>.</span>datasets<span style=color:#f92672>.</span>MNIST(root<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;data/&#39;</span>, train<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, download<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><p>首先我们先来看看 MNIST 数据长什么样。</p><p><img loading=lazy src=./image_29.png alt=image_29.png></p><p>MNIST 数据集为 0～9 的手写数字图片，有 60000 张训练样本，还有 10000 张测试样本，每张图片的分辨率为 28 * 28。
我们可以定义 28 * 28 = 784 个节点的输入层，使用 10 个节点的输出层，每个节点输出代表 0～9 各数字的决策值，输出 1 时代表为该数字。</p><h3 id=定义模型结构>定义模型结构<a hidden class=anchor aria-hidden=true href=#定义模型结构>#</a></h3><p>手写图片识别属于分类问题，因此我们可以采用逻辑回归，先定义 Sigmoid 激活函数以及其导数形式</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sigmoid <span style=color:#f92672>=</span> <span style=color:#66d9ef>lambda</span> z: <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>exp(<span style=color:#f92672>-</span>z))
</span></span><span style=display:flex><span>derivative_sigmoid <span style=color:#f92672>=</span> <span style=color:#66d9ef>lambda</span> z: sigmoid(z) <span style=color:#f92672>*</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> sigmoid(z))
</span></span></code></pre></div><p>图片特征提取属于高维度的回归问题，二维矩阵 + 矩阵特征转换，因此可以定义一个 3 层的神经网络（1层输入，2层隐含，1层输出）。训练方式使用 MBGD 算法 + BP 算法。因此我们可以简单地定义下面的的模型结构。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>NeuralNetwork</span>(object):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, l0, l1, l2, l3, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>6</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        初始化神经网络
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :param l0: 输入层节点数
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :param l1: 隐含层 l1 节点数
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :param l2: 隐含层 l2 节点数
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :param l3: 输出层节点数量
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        :param batch_size: 单次训练批次数据量
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span>  <span style=color:#75715e># 学习率</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>batch_size <span style=color:#f92672>=</span> batch_size
</span></span><span style=display:flex><span>        <span style=color:#75715e># 各层权重与偏置量</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>w1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(l0, l1) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.01</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>b1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(l1) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.01</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>w2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(l1, l2) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.01</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>b2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(l2) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.01</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>w3 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(l2, l3) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.01</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>b3 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(l3) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.01</span>
</span></span></code></pre></div><h3 id=算法实现>算法实现<a hidden class=anchor aria-hidden=true href=#算法实现>#</a></h3><p>定义了模型的结构，接下来就是前馈传播和反向传播算法的实现了，由于反向传播算法需要使用到各层在前馈时的净输入和激活值，因此前馈方法会将这些数据返回，用于反向传播。</p><p>只需要使用代码将数据代入上面提到的公式，因此实现起来很简单。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># in NeuralNetwork</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    向前传播推导结果
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    :param x: 输入的 [784] 向量矩阵
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    :return: 输出各层的净输入和激活值
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    z1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(x, self<span style=color:#f92672>.</span>w1) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b1
</span></span><span style=display:flex><span>    o1 <span style=color:#f92672>=</span> sigmoid(z1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    z2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(o1, self<span style=color:#f92672>.</span>w2) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b2
</span></span><span style=display:flex><span>    o2 <span style=color:#f92672>=</span> sigmoid(z2)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    z3 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(o2, self<span style=color:#f92672>.</span>w3) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b3
</span></span><span style=display:flex><span>    o3 <span style=color:#f92672>=</span> sigmoid(z3)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> z1, o1, z2, o2, z3, o3
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># in NeuralNetwork</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>backward</span>(self, x, z1, o1, z2, o2, err3):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    反向传播更新权重
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    dot_w3 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(o2<span style=color:#f92672>.</span>T, err3) <span style=color:#f92672>/</span> self<span style=color:#f92672>.</span>batch_size
</span></span><span style=display:flex><span>    dot_b3 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(err3, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>) <span style=color:#f92672>/</span> self<span style=color:#f92672>.</span>batch_size
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    err2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(err3, self<span style=color:#f92672>.</span>w3<span style=color:#f92672>.</span>T) <span style=color:#f92672>*</span> derivative_sigmoid(z2)
</span></span><span style=display:flex><span>    dot_w2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(o1<span style=color:#f92672>.</span>T, err2) <span style=color:#f92672>/</span> self<span style=color:#f92672>.</span>batch_size
</span></span><span style=display:flex><span>    dot_b2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(err2, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>) <span style=color:#f92672>/</span> self<span style=color:#f92672>.</span>batch_size
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    err1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(err2, self<span style=color:#f92672>.</span>w2<span style=color:#f92672>.</span>T) <span style=color:#f92672>*</span> derivative_sigmoid(z1)
</span></span><span style=display:flex><span>    dot_w1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(x<span style=color:#f92672>.</span>T, err1) <span style=color:#f92672>/</span> self<span style=color:#f92672>.</span>batch_size
</span></span><span style=display:flex><span>    dot_b1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(err1, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>) <span style=color:#f92672>/</span> self<span style=color:#f92672>.</span>batch_size
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>w3 <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> dot_w3
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>b3 <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> dot_b3
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>w2 <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> dot_w2
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>b2 <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> dot_b2
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>w1 <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> dot_w1
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>b1 <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> dot_b1
</span></span></code></pre></div><h3 id=训练与测试>训练与测试<a hidden class=anchor aria-hidden=true href=#训练与测试>#</a></h3><p>最后编写训练和测试的方法，测试时使用完成训练的模型。由于我们采用小批次梯度，因此取数据时需要按照每个批次的数据量来取。另外一次样本的遍历可能不足以让模型得到很好的效果，因此我们可以进行多次全样本的训练。训练时我们也可以计算一下当前批次的损失，来观察模型的拟合情况。</p><p>测试时采用一次性计算，得到所有测试数据的结果矩阵，对预测结果获取最大值索引，也就是单个样本预测结果为 1 的位置，该位置为样本的预测结果数值，最后计算样本预测值和预期值的匹配数量，来得到准确率。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train</span>(nn, data, targets):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> cou <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>60000</span>, nn<span style=color:#f92672>.</span>batch_size):
</span></span><span style=display:flex><span>            x <span style=color:#f92672>=</span> data[i:i <span style=color:#f92672>+</span> nn<span style=color:#f92672>.</span>batch_size]
</span></span><span style=display:flex><span>            y <span style=color:#f92672>=</span> targets[i:i <span style=color:#f92672>+</span> nn<span style=color:#f92672>.</span>batch_size]
</span></span><span style=display:flex><span>            z1, o1, z2, o2, z3, o3 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>forward(x)
</span></span><span style=display:flex><span>            err3 <span style=color:#f92672>=</span> (o3 <span style=color:#f92672>-</span> y) <span style=color:#f92672>*</span> derivative_sigmoid(z3)
</span></span><span style=display:flex><span>            loss <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum((o3 <span style=color:#f92672>-</span> y) <span style=color:#f92672>*</span> (o3 <span style=color:#f92672>-</span> y)) <span style=color:#f92672>/</span> nn<span style=color:#f92672>.</span>batch_size
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>&#34;cou:&#34;</span> <span style=color:#f92672>+</span> str(cou) <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;, err:&#34;</span> <span style=color:#f92672>+</span> str(loss))
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>backward(x, z1, o1, z2, o2, err3)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test</span>(nn, data, targets):
</span></span><span style=display:flex><span>    _, _, _, _, _, o3 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>forward(data)
</span></span><span style=display:flex><span>    result <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argmax(o3, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    precision <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(result <span style=color:#f92672>==</span> targets) <span style=color:#f92672>/</span> <span style=color:#ae81ff>10000</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Precision:&#34;</span>, precision)
</span></span></code></pre></div><h3 id=数据预处理>数据预处理<a hidden class=anchor aria-hidden=true href=#数据预处理>#</a></h3><p>在获取数据时，我们需要先将图片二维的像素数据平铺成一维矩阵，将对应的数字标签 0～9 转换成 1 维矩阵的输出，例如 3 转换为 [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]。由于测试数据的标签不需要参与反向传播，我们不做矩阵转换，这样可以方便我们对预测结果做计算。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>target_matrix</span>(targets):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    数字标签转换
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    :param targets: 对于的数字标签矩阵
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    :return:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    num <span style=color:#f92672>=</span> len(targets)
</span></span><span style=display:flex><span>    result <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((num, <span style=color:#ae81ff>10</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(num):
</span></span><span style=display:flex><span>        result[i][targets[i]] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> result
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#75715e># 训练数据</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_train_data</span>():
</span></span><span style=display:flex><span>    train_data <span style=color:#f92672>=</span> torchvision<span style=color:#f92672>.</span>datasets<span style=color:#f92672>.</span>MNIST(root<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;data/&#39;</span>, train<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, download<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Numpy 矩阵转换</span>
</span></span><span style=display:flex><span>    train_data<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> train_data<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>numpy()  <span style=color:#75715e># [60000,28,28]</span>
</span></span><span style=display:flex><span>    train_data<span style=color:#f92672>.</span>targets <span style=color:#f92672>=</span> train_data<span style=color:#f92672>.</span>targets<span style=color:#f92672>.</span>numpy()  <span style=color:#75715e># [60000]</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 输入向量处理，将二维数据平铺</span>
</span></span><span style=display:flex><span>    train_data<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> train_data<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>reshape(<span style=color:#ae81ff>60000</span>, <span style=color:#ae81ff>28</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>28</span>) <span style=color:#f92672>/</span> <span style=color:#ae81ff>255.</span>  <span style=color:#75715e># (60000, 784)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 标签转换</span>
</span></span><span style=display:flex><span>    train_data<span style=color:#f92672>.</span>targets <span style=color:#f92672>=</span> target_matrix(train_data<span style=color:#f92672>.</span>targets)  <span style=color:#75715e># (60000, 10)</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> train_data
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 测试数据</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_test_data</span>():
</span></span><span style=display:flex><span>    test_data <span style=color:#f92672>=</span> torchvision<span style=color:#f92672>.</span>datasets<span style=color:#f92672>.</span>MNIST(root<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;data/&#39;</span>, train<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    test_data<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> test_data<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>numpy()  <span style=color:#75715e># [10000,28,28]</span>
</span></span><span style=display:flex><span>    test_data<span style=color:#f92672>.</span>targets <span style=color:#f92672>=</span> test_data<span style=color:#f92672>.</span>targets<span style=color:#f92672>.</span>numpy()  <span style=color:#75715e># [10000]</span>
</span></span><span style=display:flex><span>    test_data<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> test_data<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>reshape(<span style=color:#ae81ff>10000</span>, <span style=color:#ae81ff>28</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>28</span>) <span style=color:#f92672>/</span> <span style=color:#ae81ff>255.</span>  <span style=color:#75715e># (10000, 784)</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> test_data
</span></span></code></pre></div><p>最后把上面的步骤组织起来</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>demo</span>():
</span></span><span style=display:flex><span>    nn <span style=color:#f92672>=</span> NeuralNetwork(<span style=color:#ae81ff>784</span>, <span style=color:#ae81ff>200</span>, <span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>    train_data <span style=color:#f92672>=</span> load_train_data()
</span></span><span style=display:flex><span>    train(nn, train_data<span style=color:#f92672>.</span>data, train_data<span style=color:#f92672>.</span>targets)
</span></span><span style=display:flex><span>    test_data <span style=color:#f92672>=</span> load_test_data()
</span></span><span style=display:flex><span>    test(nn, test_data<span style=color:#f92672>.</span>data, test_data<span style=color:#f92672>.</span>targets)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>demo()
</span></span></code></pre></div><p>最终测试结果 Precision: 0.9686，即准确率有 96%。</p><p>代码地址：<a href=https://github.com/korilin/neural_network_tech_sharing>https://github.com/korilin/neural_network_tech_sharing</a></p><h2 id=其他网络模型设计>其他网络模型设计<a hidden class=anchor aria-hidden=true href=#其他网络模型设计>#</a></h2><p>除了前馈神经网络外，神经网络的节点类型还有很多，不同模型的训练/处理消耗的资源，以及应用场景也不一样。</p><h3 id=深度神经网络>深度神经网络<a hidden class=anchor aria-hidden=true href=#深度神经网络>#</a></h3><p>在机器学习中有一个深度学习话题，在神经网络中，深度学习体现在网络隐含层数量，层数多的网络称为深度神经网络（DNN），像深度残差学习网络最多能有 152 层，但随着网络增加训练难度也非常大。</p><h3 id=卷积神经网络>卷积神经网络<a hidden class=anchor aria-hidden=true href=#卷积神经网络>#</a></h3><p>卷积神经网络（CNN）也是比较常用的网络结构，CNN 在图片识别上有非常理想的效果，许多图片、视频的识别/分类模型都采用了 CNN 的思想。</p><p>CNN 的核心为卷积核，在图像中有着颜色通道，局部特征等特有特性，这些特征的特点来自相邻像素的关联性，通常这些特征在局部或整体上会给人带来不同的感受。因此出现了采用卷积来提取局部特征的方式，在数学算子上卷积是通过两个函数生成第三个函数，表征函数 f 与经过翻转和平移的 g 的乘积函数所围成的曲边梯形的面积。函数 g 在卷积神经网络中也称为滤波器。</p><p><img loading=lazy src=./image_30.png alt="3Blue1Brown 视频的截图">
<img loading=lazy src=./image_31.png alt="3Blue1Brown 视频的截图"></p><h3 id=循环神经网络>循环神经网络<a hidden class=anchor aria-hidden=true href=#循环神经网络>#</a></h3><p>在普通神经网络和 CNN 中，每层神经元的信号只能向上层传播，但这样对时间序列上的变化进行建模会比较困难，循环神经网络（RNN）就是为了适应这种需求出现的。
RNN 中的神经元的输出可以在下一个时间戳中直接作用到自身，也就是每个神经元的输入，出了上一层的输入外，还结合了神经元自身的上一次输入，用图表示如下<img loading=lazy src=./image_32.png alt="来自 https://www.zhihu.com/question/34681168/answer/84061846"></p><h3 id=生成对抗网络>生成对抗网络<a hidden class=anchor aria-hidden=true href=#生成对抗网络>#</a></h3><p>生成对抗网络（GAN）属于生成模型的一种，由一个生成网络和一个判别网络组成。
生成网络模仿真实样本生成假数据来给判别网络识别，判别网络则识别输入数据为真实样本还是生成网络生成的假数据。两个网络之间通过相互对抗和参数调整，最终使得判别网络无法判断生成网络的输出是否为真实数据。
GAN 是非监督式学习的一种，但实际上在半监督、强化学习中也有效果。
GAN 还有一个变体 DCGAN，由于 CNN 中卷积核对图片特征的提取具有非常好的效果，因此 DCGAN 中使用了反卷积来对特征进行反推，来对特征进行更好转换输出。</p><h2 id=参考>参考<a hidden class=anchor aria-hidden=true href=#参考>#</a></h2><ol><li><a href=https://blog.csdn.net/weixin_39910711/article/details/81607386>机器学习算法（一）：逻辑回归模型（Logistic Regression, LR）_意念回复的博客-CSDN博客</a></li><li><a href=https://nndl.github.io/nndl-book.pdf>神经网络与深度学习</a></li><li><a href=https://www.heatonresearch.com/2017/06/01/hidden-layers.html>The Number of Hidden Layers</a></li><li><a href=https://zhuanlan.zhihu.com/p/364620596>深度学习笔记：如何理解激活函数？（附常用激活函数）</a></li><li><a href=https://blog.csdn.net/woshicao11/article/details/81458972>机器学习之Logistic回归激活函数为什么是Sigmoid？_logistic回归的激活函数_MuBaicao的博客-CSDN博客</a></li><li><a href=https://www.cnblogs.com/shixiangwan/p/7532830.html>常见的几种最优化方法（梯度下降法、牛顿法、拟牛顿法、共轭梯度法等） - 蓝鲸王子 - 博客园</a></li><li><a href=https://blog.csdn.net/weixin_39910711/article/details/124310557>卷积神经网络中二维卷积核与三维卷积核有什么区别？_3d卷积和2d卷积区别_意念回复的博客-CSDN博客</a></li><li><a href=https://blog.csdn.net/weixin_39910711/article/details/89483662>生成模型（Generative）和判别模型（Discriminative）_生成模型和判别模型_意念回复的博客-CSDN博客</a></li><li><a href=https://www.zhihu.com/question/34681168/answer/84061846>CNN（卷积神经网络）、RNN（循环神经网络）、DNN（深度神经网络）的内部网络结构有什么区别？ - 知乎</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=http://answerkobe.github.io/tags/neural-network/>Neural Network</a></li><li><a href=http://answerkobe.github.io/tags/python/>Python</a></li></ul><nav class=paginav><a class=next href=http://answerkobe.github.io/posts/design-pattern/><span class=title>Next »</span><br><span>Java设计模式</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 神经网络基础理论与简单实践 on x" href="https://x.com/intent/tweet/?text=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%9f%ba%e7%a1%80%e7%90%86%e8%ae%ba%e4%b8%8e%e7%ae%80%e5%8d%95%e5%ae%9e%e8%b7%b5&amp;url=http%3a%2f%2fanswerkobe.github.io%2fposts%2fbasic-theory-and-simple-practice-of-neural-network%2f&amp;hashtags=NeuralNetwork%2cPython"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 神经网络基础理论与简单实践 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2fanswerkobe.github.io%2fposts%2fbasic-theory-and-simple-practice-of-neural-network%2f&amp;title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%9f%ba%e7%a1%80%e7%90%86%e8%ae%ba%e4%b8%8e%e7%ae%80%e5%8d%95%e5%ae%9e%e8%b7%b5&amp;summary=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%9f%ba%e7%a1%80%e7%90%86%e8%ae%ba%e4%b8%8e%e7%ae%80%e5%8d%95%e5%ae%9e%e8%b7%b5&amp;source=http%3a%2f%2fanswerkobe.github.io%2fposts%2fbasic-theory-and-simple-practice-of-neural-network%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 神经网络基础理论与简单实践 on reddit" href="https://reddit.com/submit?url=http%3a%2f%2fanswerkobe.github.io%2fposts%2fbasic-theory-and-simple-practice-of-neural-network%2f&title=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%9f%ba%e7%a1%80%e7%90%86%e8%ae%ba%e4%b8%8e%e7%ae%80%e5%8d%95%e5%ae%9e%e8%b7%b5"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 神经网络基础理论与简单实践 on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2fanswerkobe.github.io%2fposts%2fbasic-theory-and-simple-practice-of-neural-network%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 神经网络基础理论与简单实践 on whatsapp" href="https://api.whatsapp.com/send?text=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%9f%ba%e7%a1%80%e7%90%86%e8%ae%ba%e4%b8%8e%e7%ae%80%e5%8d%95%e5%ae%9e%e8%b7%b5%20-%20http%3a%2f%2fanswerkobe.github.io%2fposts%2fbasic-theory-and-simple-practice-of-neural-network%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 神经网络基础理论与简单实践 on telegram" href="https://telegram.me/share/url?text=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%9f%ba%e7%a1%80%e7%90%86%e8%ae%ba%e4%b8%8e%e7%ae%80%e5%8d%95%e5%ae%9e%e8%b7%b5&amp;url=http%3a%2f%2fanswerkobe.github.io%2fposts%2fbasic-theory-and-simple-practice-of-neural-network%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 神经网络基础理论与简单实践 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%9f%ba%e7%a1%80%e7%90%86%e8%ae%ba%e4%b8%8e%e7%ae%80%e5%8d%95%e5%ae%9e%e8%b7%b5&u=http%3a%2f%2fanswerkobe.github.io%2fposts%2fbasic-theory-and-simple-practice-of-neural-network%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><script id=utteranc src=https://utteranc.es/client.js repo=Answerkobe/utterances issue-term=pathname theme=github-light crossorigin=anonymous async></script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.body.className.includes("dark")?"github-light":"photon-dark",t={type:"set-theme",theme:e},n=document.querySelector(".utterances-frame");n.contentWindow.postMessage(t,"https://utteranc.es")})</script></article></main><footer class=footer><span>&copy; 2023 <a href=http://answerkobe.github.io/>Iverson's blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>